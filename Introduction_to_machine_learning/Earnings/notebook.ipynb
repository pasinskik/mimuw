{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDWQsRPviGeh"
   },
   "source": [
    "# Wage Structure Analysis - Machine Learning Approaches\n",
    "\n",
    "**Author:** Kacper Pasiński, kp459461@students.mimuw.edu.pl  \n",
    "**Course:** WUM - Introduction to Machine Learning  \n",
    "**Date:** 06.05.2025\n",
    "**Dataset:** 'earnings.csv'\n",
    "\n",
    "---\n",
    "\n",
    "**Overview**  \n",
    "In this notebook, we analyze the 2010 wage structure dataset to:\n",
    "- Inspect distributions and correlations  \n",
    "- Cluster employees using K-Means  \n",
    "- Classify higher‐education status  \n",
    "- Predict base salary with ensemble models and bootstrap intervals  \n",
    "\n",
    "We employ a mix of statistical and machine-learning methods - including log‐transforms, one‐hot encoding, cross-validation, and feature‐importance analysis - to uncover the key drivers of pay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfrROKon7B1k"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NFmsNZ567CDb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m skew, kurtosis\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, accuracy_score, roc_auc_score, classification_report, ConfusionMatrixDisplay, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkktoFu766s6"
   },
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "PPb_r4nj65lr",
    "outputId": "e50d08f9-5df7-4d7d-aa08-00cd80b2cc84"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('earnings.csv', sep=';')\n",
    "print(f\"Loaded DataFrame with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QxBhWUC7x6Y"
   },
   "source": [
    "## Task 1: Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SkXjb2QB7z5U",
    "outputId": "014d6bab-9c47-46a4-9bdd-f9e09dc8d01c"
   },
   "outputs": [],
   "source": [
    "  # Basic info\n",
    "  print(\"Shape:\", df.shape)\n",
    "  print(\"\\nData types:\\n\", df.dtypes)\n",
    "  print(\"\\nMissing values per column:\\n\", df.isna().sum())\n",
    "\n",
    "  # Numeric summary with skewness & kurtosis\n",
    "  numeric_cols = df.select_dtypes(include=[np.number]).columns.drop('id')\n",
    "  desc = df[numeric_cols].describe().T\n",
    "  desc['skewness'] = df[numeric_cols].apply(skew).round(2)\n",
    "  desc['kurtosis'] = df[numeric_cols].apply(kurtosis).round(2)\n",
    "  print(\"\\nNumeric summary:\\n\", desc)\n",
    "\n",
    "  # Categorical distributions\n",
    "  cat_cols = ['sector','section_07','sex','education','contract']\n",
    "  print()\n",
    "  for c in cat_cols:\n",
    "      freqs = df[c].value_counts(normalize=True).mul(100).round(1)\n",
    "      print(f\"{c} (% of total):\\n{freqs}\\n\")\n",
    "\n",
    "  # Correlation heatmap\n",
    "  plt.figure(figsize=(10,8))\n",
    "  sns.heatmap(df[numeric_cols].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "  plt.title(\"Numeric Feature Correlations\")\n",
    "  plt.show()\n",
    "\n",
    "  # Histograms of all numeric features\n",
    "  df[numeric_cols].hist(bins=30, figsize=(12, 8))\n",
    "  plt.suptitle(\"Histograms of Numeric Features\")\n",
    "  plt.show()\n",
    "\n",
    "  # Log-histograms for skewed features\n",
    "  skewed = [c for c in numeric_cols if df[c].skew() > 1]\n",
    "  for c in skewed:\n",
    "      plt.figure()\n",
    "      sns.histplot(np.log1p(df[c]), kde=True)\n",
    "      plt.title(f\"{c} (log1p-transformed)\")\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s66BdvEyDVnu"
   },
   "source": [
    "### Summary of the data\n",
    "\n",
    "- **Rows & columns**: There are 11 000 people and 15 columns (9 quantitative, 5 qualitative and ID).  \n",
    "- **No missing data**: Every cell is filled.  \n",
    "- **Categories**: Most people work in the public sector and have permanent contracts.  \n",
    "- **Numbers**: Columns like salary (`base`) and bonuses are very lopsided, which means that a few people earn much more than the rest. For most of these measures mean ≫ median, indicating right-skew.\n",
    "- **Distributions**:  \n",
    "  - For *age* and *job duration*, the values look roughly bell-shaped (close to Normal).  \n",
    "  - For *salary* and *extra pay*, the raw numbers are stretched out to the right - so we should take a log to make them look more balanced before modeling.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UV_iUFjYFjHC"
   },
   "source": [
    "## Task 2: Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyJikWv7GMHQ"
   },
   "source": [
    "**Goal:** Explore clustering of the samples.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **We shall select variables & transform**  \n",
    "   - We'll drop highly correlated pairs to avoid redundancy:  \n",
    "     - **`age`** (r≈0.88 with `duration_total`)  \n",
    "     - **`duration_overtime`** (r≈0.87 with `overtime_pay`)  \n",
    "   - Using all remaining **numeric** features.  \n",
    "   - **Log-transforming** the skewed pay columns (`base`, `bonus`, `overtime_pay`, `other`).  \n",
    "   - **Standardizing** everything to mean 0, std 1.\n",
    "\n",
    "2. **We cluster with K-Means**  \n",
    "   - K-Means uses Euclidean distance on scaled data - good when features are on the same scale.\n",
    "\n",
    "3. **We will have to choose optimal k**  \n",
    "   - Trying k = 2…10 and picking the one with the highest average **silhouette score** (measures how well each point fits its own cluster vs. others).\n",
    "\n",
    "4. **We visualize clusters**  \n",
    "   - Projecting into 2D **PCA space** to confirm visually that clusters are well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "rB6GI4i9LyjX",
    "outputId": "c979e3e4-005a-4186-97a3-f49853c97d01"
   },
   "outputs": [],
   "source": [
    "# Identify & filter numeric columns\n",
    "num_cols = (\n",
    "    df\n",
    "    .select_dtypes(include='number')\n",
    "    .columns\n",
    "    .drop(['id', 'age', 'duration_overtime'])  # drop ID and redundant features\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Split earnings vs. other numerics\n",
    "earn_cols  = ['base','bonus','overtime_pay','other']\n",
    "other_num  = [c for c in num_cols if c not in earn_cols]\n",
    "\n",
    "# Build transformer: log1p for earnings, then scale; scale the rest\n",
    "preproc = ColumnTransformer([\n",
    "    ('earn', Pipeline([\n",
    "        ('log',   FunctionTransformer(np.log1p, feature_names_out='one-to-one')),\n",
    "        ('scale', StandardScaler())\n",
    "    ]), earn_cols),\n",
    "    ('other', StandardScaler(), other_num)\n",
    "])\n",
    "\n",
    "X_scaled = preproc.fit_transform(df[earn_cols + other_num])\n",
    "\n",
    "# Find optimal k by silhouette\n",
    "sil_scores = []\n",
    "K_range    = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    km     = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labs   = km.fit_predict(X_scaled)\n",
    "    sil    = silhouette_score(X_scaled, labs)\n",
    "    sil_scores.append(sil)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.lineplot(x=list(K_range), y=sil_scores, marker='o')\n",
    "plt.xticks(list(K_range))\n",
    "plt.xlabel('Number of clusters k')\n",
    "plt.ylabel('Avg. silhouette score')\n",
    "plt.title('Choosing k via silhouette')\n",
    "plt.show()\n",
    "\n",
    "best_k = K_range[np.argmax(sil_scores)]\n",
    "print(\"Optimal k:\", best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "plvF6i7RMBsC",
    "outputId": "951ca4d2-88b6-4e7f-9699-d91168cd7885"
   },
   "outputs": [],
   "source": [
    "# We found the optimal k (which is 6), let's fit final KMeans and summarize\n",
    "best_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "df['cluster'] = best_kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Cluster sizes\n",
    "print(\"Cluster sizes:\")\n",
    "print(df['cluster'].value_counts().sort_index(), \"\\n\")\n",
    "\n",
    "# Cluster centers (in original scale) by averaging back on df\n",
    "cluster_means = df.groupby('cluster')[earn_cols + other_num].mean().round(2)\n",
    "print(\"Cluster means:\")\n",
    "display(cluster_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "L58p2H9zMPsv",
    "outputId": "85e8e179-1135-4839-b31d-d278e0b57272"
   },
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D PCA space\n",
    "pca    = PCA(n_components=2, random_state=42)\n",
    "X_pca  = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=df['cluster'],\n",
    "                palette='tab10', s=30, alpha=0.7)\n",
    "plt.title('Clusters on first two PCA components')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcB86BWc94W5"
   },
   "source": [
    "### Justification of the approach\n",
    "\n",
    "- **Variable selection:** we retained only the truly informative **numeric** columns - namely the pay components (`base`, `bonus`, `overtime_pay`, `other`) and the three non-redundant tenure measures (`duration_total`, `duration_entity`, `duration_nominal`) - and dropped the technical `id` plus the two highly correlated features (`age` and `duration_overtime`). This leaves a concise, continuous feature set ideal for K-Means with Euclidean distance.\n",
    "- **Skewed-pay transformations:** the pay variables are highly right-skewed (long tails, large outliers), so we applied a **log1p** transform to each of them.  This compresses extreme values, brings distributions closer to symmetric, and prevents a few high earners from dominating the distance calculations.\n",
    "- **Scaling:** after log-transforming pay, we ran **StandardScaler** on the numeric features so each has mean 0 and standard deviation 1.  Without this, variables on larger scales (e.g. salary vs. age) would unduly influence clustering.\n",
    "- **Clustering algorithm:** with all features on the same (standardized) scale, we applied **K-Means** and chose the number of clusters via the **average silhouette score** over k = 2…10.  This assures that our final clusters are both compact (low within-cluster distance) and well separated from each other.\n",
    "- **Visualization in PCA space:** gives us a 2D “map” of the high-dimensional data that preserves most of its natural variation. Plotting clusters there helps visually confirm whether our clustering has found distinct, meaningful groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO8xvxOVOvBP"
   },
   "source": [
    "## Task 3: Classification of higher education status\n",
    "\n",
    "**Goal:** Predict whether an employee has a higher-education degree (`education <= 2`) using the other variables.\n",
    "\n",
    "**Steps:**\n",
    "1. We'll **create** the binary target `higher_edu`.\n",
    "2. **Selecting** features and **dropping** redundant columns (`id`, `education`, `higher_edu`, `age`, `duration_overtime`).\n",
    "3. **Building** preprocessing + model pipelines for three classifiers.\n",
    "4. **Running** 5-fold stratified cross-validation to estimate **accuracy** and **ROC AUC**.\n",
    "5. **Refitting** the best model on an 80/20 train/test split and **evaluate** on held-out data.\n",
    "6. **Inspecting** variable importance for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8sUByVCgQJQf",
    "outputId": "991a14e2-057d-4fcd-f665-821490acbbbd"
   },
   "outputs": [],
   "source": [
    "# Define target and feature sets\n",
    "df2 = pd.read_csv('earnings.csv', sep=';')\n",
    "df2['higher_edu'] = (df2['education'] <= 2).astype(int)\n",
    "\n",
    "# Feature selection and splitting\n",
    "X = df2.drop(columns=[\n",
    "    'id', 'education', 'higher_edu',   # technical & target\n",
    "    'age', 'duration_overtime'         # highly correlated duplicates\n",
    "])\n",
    "y = df2['higher_edu']\n",
    "\n",
    "# Identify feature types\n",
    "earn_cols = ['base', 'bonus', 'overtime_pay', 'other']  # skewed pay\n",
    "num_cols  = [c for c in X.select_dtypes(include='number').columns if c not in earn_cols]\n",
    "cat_cols  = ['sector', 'section_07', 'sex', 'contract'] # categorical codes\n",
    "\n",
    "# Build preprocessing transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    # Log1p + standard-scale the skewed earnings\n",
    "    ('earn', Pipeline([\n",
    "        ('log1p', FunctionTransformer(np.log1p, feature_names_out='one-to-one')),\n",
    "        ('scale', StandardScaler())\n",
    "    ]), earn_cols),\n",
    "    # Standard-scale the other numerics\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    # One-hot encode categoricals\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "# Compare classifiers with 5-fold CV\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest':       RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting':   GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline([('prep', preprocessor), ('clf', clf)])\n",
    "    acc = cross_val_score(pipe, X, y, cv=cv, scoring='accuracy')\n",
    "    roc = cross_val_score(pipe, X, y, cv=cv, scoring='roc_auc')\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Acc Mean': f\"{acc.mean():.3f}\",\n",
    "        'Acc Std' : f\"{acc.std():.3f}\",\n",
    "        'ROC AUC Mean': f\"{roc.mean():.3f}\",\n",
    "        'ROC AUC Std' : f\"{roc.std():.3f}\"\n",
    "    })\n",
    "\n",
    "cv_results = pd.DataFrame(results).set_index('Model')\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz2uUJk6QANH"
   },
   "source": [
    "### 3.1 Refit Best Model on Hold-Out Test Set & Evaluate\n",
    "\n",
    "Based on cross-validation, **Gradient Boosting** had the highest ROC AUC (0.951).  \n",
    "In this part we will:\n",
    "\n",
    "1. Split out 20 % of data as a **test set** (stratified by `higher_edu`).  \n",
    "2. Refit the full pipeline (preprocessing + classifier) on the 80 % training data.  \n",
    "3. Predict on the test set and report **accuracy**, **ROC AUC**, and a **classification report**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "6dp0t3kBSc2H",
    "outputId": "c634350a-99ac-473f-b992-2635463b0ee3"
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "# Build & fit pipeline with the chosen model\n",
    "best_pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('clf', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred_proba = best_pipe.predict_proba(X_test)[:,1]\n",
    "y_pred       = best_pipe.predict(X_test)\n",
    "\n",
    "print(\"Test accuracy :\", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Test ROC AUC  :\", round(roc_auc_score(y_test, y_pred_proba), 3))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "ConfusionMatrixDisplay.from_estimator(best_pipe, X_test, y_test, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW0LdTk3TnCL"
   },
   "source": [
    "### 3.2 Variable Importance\n",
    "\n",
    "For the fitted Gradient Boosting model, we can look at the `.feature_importances_` array.  \n",
    "We’ll extract the transformed feature names from the preprocessing step so that the importances line up with real variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "OEhPlkAcT8qy",
    "outputId": "d6675eeb-b1f5-4876-822b-6a67dbcd4446"
   },
   "outputs": [],
   "source": [
    "# Get feature names out of the ColumnTransformer\n",
    "feat_names = best_pipe.named_steps['prep'].get_feature_names_out()\n",
    "\n",
    "# Grab importances from the final estimator\n",
    "importances = best_pipe.named_steps['clf'].feature_importances_\n",
    "\n",
    "# Build a sorted DataFrame\n",
    "imp_df = (\n",
    "    pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
    "      .sort_values('importance', ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Show top 10\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=imp_df.head(10), y='feature', x='importance')\n",
    "plt.title(\"Top 10 Feature Importances (Gradient Boosting)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "imp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is2_vTTMT7Mf"
   },
   "source": [
    "After comparing three classifiers with 5-fold CV, Gradient Boosting performed best (mean ROC AUC ≈ 0.951, accuracy ≈ 0.878) and, when retrained on 80 % of the data and evaluated on the 20 % hold-out set, achieved similarly strong results (test ROC AUC ≈ 0.947, accuracy ≈ 0.877).  \n",
    "\n",
    "Looking at the top feature importances from the final Gradient Boosting model:  \n",
    "1. **`duration_nominal`** (nominal contract length) was the strongest predictor, suggesting that employees with longer nominal terms are much more likely to have higher education.  \n",
    "2. **`base`** salary came next, indicating that more highly educated employees tend to earn higher base pay.  \n",
    "3. **`duration_total`** (total time with the employer) and **`other`** pay components also contributed meaningfully, implying experience and diverse pay structures are linked to education level.  \n",
    "\n",
    "All remaining features had much smaller importances, so they play a relatively minor role in the model.  \n",
    "\n",
    "**Overall**, our Gradient Boosting classifier can reliably distinguish higher-education employees using tenure and pay variables, with an expected ROC AUC around 0.95 on new data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOTlcIgzUc7c"
   },
   "source": [
    "# Task 4: Regression of Base Salary\n",
    "\n",
    "**Goal:** Predict base salary using a log-transformed target and Random Forest, with cross-validation for model selection and evaluation on a hold-out set. Use feature importances for interpretation.\n",
    "\n",
    "**Steps:**\n",
    "1. **Loading data & dropping redundancies:** we remove `id`, `base`, and also `age` and `duration_overtime` (highly correlated).  \n",
    "2. **Train/test splitting:** 80 / 20 random split on the original base salary.  \n",
    "3. **Log-transforming the target:** we define `y = log1p(base)` to reduce skew.  \n",
    "4. **One-hot encoding categorical variables:** using `drop_first=True` to avoid multicollinearity.  \n",
    "5. **Cross-validation:** we compare Random Forest and Gradient Boosting on log-scale using 5-fold CV.  \n",
    "6. **Refitting the best model (Random Forest):** we train on the full training set.  \n",
    "7. **Evaluating on hold-out test set:** we back-transform predictions and compute RMSE (PLN) and R².  \n",
    "8. **Inspecting feature importances:** we report top predictors based on the trained Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSwCakNBUSO3",
    "outputId": "556b90a7-c31d-4228-bb00-87da40b5fec5"
   },
   "outputs": [],
   "source": [
    "# Define target and drop redundant features\n",
    "df3 = pd.read_csv('earnings.csv', sep=';')\n",
    "df3['log_base'] = np.log1p(df3['base'])\n",
    "X = df3.drop(columns=['id', 'base', 'age', 'log_base', 'duration_overtime'])\n",
    "y = df3['log_base']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocessing pipeline\n",
    "earn_cols = ['bonus','overtime_pay','other']\n",
    "cat_cols  = ['sector','section_07','sex','education','contract']\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "   # Log1p transform only earnings, leave others raw\n",
    "   ('earn', FunctionTransformer(np.log1p, feature_names_out='one-to-one'), earn_cols),\n",
    "   # One-hot encode everything else\n",
    "   ('cat',  OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "], remainder='passthrough')  # Pass durations through unchanged\n",
    "\n",
    "# Model pipelines & CV comparison\n",
    "models = {\n",
    "    'Random Forest':     RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([('pre', preproc), ('reg', model)])\n",
    "    # Use neg RMSE so we can get RMSE back\n",
    "    neg_rmse = cross_val_score(pipe, X_train, y_train,\n",
    "                               cv=5,\n",
    "                               scoring='neg_root_mean_squared_error')\n",
    "    r2       = cross_val_score(pipe, X_train, y_train,\n",
    "                               cv=5,\n",
    "                               scoring='r2')\n",
    "    print(f\"{name}:\")\n",
    "    print(\"  RMSE (cv mean ± std):\", -neg_rmse.mean(), \"±\", neg_rmse.std())\n",
    "    print(\"  R²   (cv mean ± std):\",  r2.mean(), \"±\", r2.std())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4V9XmOGdaFB"
   },
   "source": [
    "### 4.1 Refit Random Forest on Full Training Set & Evaluate on Hold-Out\n",
    "\n",
    "We saw that Random Forest gave slightly better CV performance. Now we:\n",
    "\n",
    "1. Refit the same pipeline on the 80% training data.  \n",
    "2. Predict on the 20% test data (remember our target is `log1p(base)`), then back-transform with `expm1`.  \n",
    "3. Compute **RMSE** in PLN and **R²** on the original salary scale (not the logarithmic one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCnXntwqR34l",
    "outputId": "1116e861-d79e-4cbf-b5c6-3d8cb0085b86"
   },
   "outputs": [],
   "source": [
    "# Rebuild and fit pipeline\n",
    "rf_pipe = Pipeline([\n",
    "    ('pre', preproc),\n",
    "    ('reg', RandomForestRegressor(n_estimators=200, random_state=42))\n",
    "])\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict and back-transform\n",
    "y_pred_log = rf_pipe.predict(X_test)        # Predicts log1p(base)\n",
    "y_pred_pln = np.expm1(y_pred_log)           # Back to base in PLN\n",
    "\n",
    "# Evaluate on the original scale\n",
    "mse_pln = mean_squared_error(np.expm1(y_test), y_pred_pln)\n",
    "rmse_pln = np.sqrt(mse_pln)\n",
    "r2_pln   = r2_score(np.expm1(y_test), y_pred_pln)\n",
    "\n",
    "print(f\"Test RMSE: {rmse_pln:.2f} PLN\")\n",
    "print(f\"Test R²  : {r2_pln:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx1Lx_S-NUzv"
   },
   "source": [
    "### 4.2 Feature importance\n",
    "\n",
    "Finally, we rank the top 10 predictors by their importance in the Random Forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "Oyz60b0MNZw2",
    "outputId": "13ec91b3-ac87-49ee-98f1-d67a994ef8b8"
   },
   "outputs": [],
   "source": [
    "# Get transformed feature names\n",
    "feat_names = rf_pipe.named_steps['pre'].get_feature_names_out()\n",
    "\n",
    "# Extract importances\n",
    "importances = rf_pipe.named_steps['reg'].feature_importances_\n",
    "\n",
    "# Build DataFrame\n",
    "imp_df = (\n",
    "    pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
    "      .sort_values('importance', ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Plot top 10\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=imp_df.head(10), x='importance', y='feature')\n",
    "plt.title(\"Top 10 Random Forest Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "imp_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMuhUqiZPM4A"
   },
   "source": [
    "### 4.3 Summary and conclusions\n",
    "\n",
    "**Model & Performance**  \n",
    "- We settled on a **Random Forest** regressor trained on  \n",
    "  - **Target:** $ \\log(1 + \\text{base salary}) $, then back-transformed to PLN  \n",
    "  - **Features:**  \n",
    "    - Skewed pay components (`bonus`, `overtime_pay`, `other`) log-transformed  \n",
    "    - Categorical codes one-hot encoded (drop-first)  \n",
    "    - Redundant variables (`age`, `duration_overtime`) dropped  \n",
    "  - **Train/Test Split:** 80 % / 20 %  \n",
    "- **Hold-out results** (on original PLN scale):  \n",
    "  - **RMSE:** ≈ 8 687 PLN  \n",
    "  - **R²:** ≈ 0.796  \n",
    "\n",
    "---\n",
    "\n",
    "### Top 10 Feature Importances\n",
    "\n",
    "| Rank | Feature                       | Importance | Role            |\n",
    "|-----:|-------------------------------|-----------:|-----------------|\n",
    "|  1   | `earn__other`                 | 0.434      | **Strong positive**: Employees with higher “other” pay components tend to earn notably higher base salaries. |\n",
    "|  2   | `remainder__duration_nominal` | 0.431      | **Strong positive**: Longer nominal contract length is a major driver of higher base pay. |\n",
    "|  3   | `remainder__duration_entity`  | 0.028      | **Positive**: More total time at the current employer slightly raises salaries. |\n",
    "|  4   | `remainder__duration_total`   | 0.027      | **Positive**: Overall tenure also contributes positively (though less than nominal duration). |\n",
    "|  5   | `earn__bonus`                 | 0.025      | **Positive**: Larger bonuses correlate with modestly higher base pay. |\n",
    "|  6   | `earn__overtime_pay`          | 0.014      | **Positive**: Paid overtime adds a small boost to base salary predictions. |\n",
    "|  7   | `cat__education_2`            | 0.012      | **Minor effect**: This education level slightly adjusts salary relative to the baseline category. |\n",
    "|  8   | `cat__education_1`            | 0.005      | **Minor effect**: The first drop-first category has a small adjustment. |\n",
    "|  9   | `cat__education_5`            | 0.004      | **Minor effect**: Another education level dummy with a tiny role. |\n",
    "| 10   | `cat__education_6`            | 0.003      | **Minor effect**: Highest education level has a very small impact. |\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion of Variable Roles (Positive vs. Negative)\n",
    "\n",
    "- **Pay components (`other`, `bonus`, `overtime_pay`)** all have **positive** associations: higher values in these earnings streams signal a higher base salary.\n",
    "- **Tenure measures** (`duration_nominal`, `duration_entity`, `duration_total`) are also **positive**: longer contracts and longer service correlate with higher pay.\n",
    "- **Education dummies** (`cat__education_i`):  \n",
    "  - These capture small **upward or downward adjustments** relative to the omitted (baseline) education category. Their low importances reflect that once pay & tenure are known, education level adds only a minor tweak.\n",
    "- **Note on sign interpretation in tree models:** feature importance in a Random Forest doesn’t directly give a sign. However, from our earlier linear‐model diagnostics we know that:  \n",
    "  - Lower numeric education codes (higher academic degrees) tended to **increase** salary;  \n",
    "  - Temporary contracts (if encoded) would have a **negative** effect.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
